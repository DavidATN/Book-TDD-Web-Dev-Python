[[chapter_11_ansible]]
== Infrastructure As Code: Automated Deployments With Ansbile

[quote, 'Cay Horstman']
______________________________________________________________
Automate, automate, automate.
______________________________________________________________

((("deployment", "automating with Ansible", id="Dfarbric11")))
((("infrastructure as code")))
In this chapter we're going to spin up an actual server,
make it accessible on the Internet with a real domain name,
and then we're going to install our app on it, using our container.

We _could_ do all these things manually,
but a key insight of the modern infrastructure management
is that automation really pays off in reducing maintenance burdens.

It's also key to making sure our tests give us true confidence over our deployments.
If we go to the trouble of building a staging server,footnote:[
What I'm calling a "staging" server, some people would
call a "development" server, and some others would also like to distinguish
"preproduction" servers.  Whatever we call it, the point is to have
somewhere we can try our code out in an environment that's as similar as
possible to the real production server.]
we want to make sure that it's as similar as possible to the production environment.
By automating the way we deploy, and using the same automation for staging and prod,
we give ourselves much more confidence.

The buzzword for automating your deployments these days is "Infrastructure as Code".

NOTE: Why not ping me a note once your site is live on the web,
    and send me the URL?
    It always gives me a warm and fuzzy feeling...
    obeythetestinggoat@gmail.com.

//TODO useful blog post https://linuxhandbook.com/autostart-podman-containers/


.ðŸš§ Warning, this chapter is heavily under construction
*******************************************************************************
As part of my work on the third edition of the book,
I'm rewriting the deployment chapters,
but this chapter is far from ready I'm afraid.
Sorry!

Following along with this chapter is going to be pretty
much impossible while I'm still half-done.

It might be best to skip ahead to <<chapter_12_organising_test_files>>

*******************************************************************************


=== Getting a Domain Name

((("domain names")))
We're going to need a couple of domain names at this point in the book--they
can both be subdomains of a single domain.  I'm going to use
_superlists.ottg.eu_ and _superlists-staging.ottg.eu_.
If you don't already own a domain, this is the time to register one!
Again, this is something I really want you to _actually_ do.
If you've never registered a domain before,
just pick any old registrar and buy a cheap one--it
should only cost you $5 or so, and you can even find free ones.
I promise seeing your site on a "real" website will be a thrill.



=== Manually Provisioning a Server to Host Our Site

TODO: revise this section

((("staging sites", "manual server provisioning", id="SSserver09")))
((("server provisioning", id="seerver09")))
We can separate out "deployment" into two tasks:

- 'Provisioning' a new server to be able to host the code
- 'Deploying' a new version of the code to an existing server

Some people like to use a brand new server for every deployment--it's what we
do at PythonAnywhere.  That's only necessary for larger, more complex sites
though, or major changes to an existing site. For a simple site like ours, it
makes sense to separate the two tasks.  And, although we eventually want both
to be completely automated, we can probably live with a manual provisioning
system for now.

As you go through this chapter, you should be aware that provisioning is
something that varies a lot, and that as a result there are few universal
best practices for deployment.  So, rather than trying to remember the
specifics of what I'm doing here, you should be trying to understand the
rationale, so that you can apply the same kind of thinking in the
specific future circumstances you encounter.


==== Choosing Where to Host Our Site


((("hosting services")))
There are loads of different solutions out there these days,
but they broadly fall into two camps:

- Running your own (probably virtual) server
- Using a ((("Platform-As-A-Service (PaaS)")))Platform-As-A-Service (PaaS)
  offering like Heroku or my old employers, PythonAnywhere.


((("PythonAnywhere")))
Particularly for small sites, a PaaS offers a lot of advantages,
and I would definitely recommend looking into them.
We're not going to use a PaaS in this book however, for several reasons.
The main reason is that I want to avoid endorsing specific commercial providers.
Secondly, all the PaaS offerings are quite different,
and the procedures to deploy to each vary a lot--learning about one
doesn't necessarily tell you about the others.
Any one of them might radically change their process or business model by the time you get to read this book.

Instead, we'll learn just a tiny bit of good old-fashioned server admin,
including SSH and manual server config.
They're unlikely to ever go away,
and knowing a bit about them will get you some respect
from all the grizzled dinosaurs out there.



==== Spinning Up a Server

I'm not going to dictate how you do this--whether you choose Amazon AWS,
Rackspace, Digital Ocean, your own server in your own data centre or a
Raspberry Pi in a cupboard under the stairs, any solution should be fine, as
long as:

* Your server is running Ubuntu 22.04 (aka "Jammy/LTS").

* You have root access to it.

* It's on the public internet.

* You can SSH into it.

I'm recommending Ubuntu as a distro because it's popular and I'm used to it.
If you know what you're doing, you can probably get away with using
something else, but you're on your own.

((("Linux servers")))
If you've never started a Linux server before and you have absolutely no idea
where to start, I wrote a
https://github.com/hjwp/Book-TDD-Web-Dev-Python/blob/master/server-quickstart.md[very brief guide on GitHub].


NOTE: Some people get to this chapter, and are tempted to skip the domain bit,
    and the "getting a real server" bit, and just use a VM on their own PC.
    Don't do this. It's _not_ the same, and you'll have more difficulty
    following the instructions, which are complicated enough as it is.
    If you're worried about cost, have a look at the link above for free options.
    ((("getting help")))


==== User Accounts, SSH, and Privileges

In these instructions, I'm assuming that you have a nonroot user account set up,
and that it has "sudo" privileges,
so whenever we need to do something that requires root access, we use sudo,
(or "become" in ansible terminology),
and I'm explicit about that in the various instructions that follow.

My user is called "elspeth", but you can call yours whatever you like!
Just remember to substitute it in all the places I've hardcoded it below.
See the guide linked above if you need tips on creating a sudo user.


.General Server Debugging Tips
*******************************************************************************

The most important lesson to remember from this chapter is,
as always but more than ever, to work incrementally,
make one change at a time, and run your tests frequently.

When things (inevitably) go wrong, resist the temptation to flail about
and make other unrelated changes in the hope that things will start working again;
instead, stop, go backward if necessary to get to a working state,
and figure out what went wrong before moving forward again.

It's just as easy to fall into the Refactoring-Cat trap on the server!

*******************************************************************************



=== Configuring Domains for Staging and Live

We don't want to be messing about with IP addresses all the time,
so we should point our staging and live domains to the server.
At my registrar, the control screens looked a bit like <<registrar-control-screens>>.

[[registrar-control-screens]]
.Domain setup
image::images/twp2_0902.png["Registrar control screens for two domains"]

//TODO: adjust illustration to show "superlists" not "book-example"

((("A-Records")))
In the DNS system, pointing a domain at a specific IP address is called an "A-Record".
All registrars are slightly different,
but a bit of clicking around should get you to the right screen in yours.
You'll need two A-records:
one for the staging address and one for the live one.
No need to worry about any other type of record.

DNS records take some time to "propagate" around the world
(it's controlled by a setting called "TTL", Time To Live),
so once you've set up your A-record,
you can check its progress on a "propagation checking" service like this one:
https://www.whatsmydns.net/#A/superlists-staging.ottg.eu.

I'm planning to host my staging server at 'superlists-staging.ottg.eu':


=== A first Cut of an Ansible Script

Infrastructure-as-code tools, also called "configuration management" tools,
come in lots of shapes and sizes.
Chef and Puppet were two of the original ones,
and you'll probably come across Terraform,
which is particularly strong on managing cloud services like AWS.

We're going to use Ansible, because it's relatively popular,
because it can do everything we need it to,
because I'm biased that it happens to be written in Python,
and because it's probably the one I'm personally most familiar with.

Another tool could probably have worked just as well!
The main thing to remember is the _concept_, which is that, as much as possible
we want to manage our server configuration _declaratively_,
by expressing the desired state of the server in a particular config syntax,
rather than specifying a procedural series of steps to be followed one by one.


Let's dip our toes into ansible,
and see if we can get it to run a simple "hello world" container on our server.

Here's what's called a "playbook" in ansible terminology.
It's in a format called YAML (Yet Another Markup Language),
which, if you've never come across before,
you will soon develop a love-hatefootnote:[
The "love" part is that yaml is very easy to _read_ and scan through at a glance.
The "hate" part is that the actual syntax is surprisingly fiddly to get right:
the difference between lists and key/value maps is subtle and I can never quite remember it honestly.]
relationship with.


[role="sourcecode"]
.infra/ansible-provision.yaml (ch11l001)
====
[source,yaml]
----
---
- hosts: all

  tasks:
    - name: Install podman  <1>
      ansible.builtin.apt:  <2>
        name: podman  <3>
        update_cache: yes
      become: true

    - name: Run container
      containers.podman.podman_container:
        name: testcontainer
        image: busybox
        state: started
        cmd: echo hello world
----
====

<1> An ansible playbook is a series of "tasks"
  (so in that sense it's still quite sequential and procedural),
  but the individual tasks themselves are quite declarative.
  Each one usually has a human-readable `name` attribute.

<2> Each tasks uses an ansible "module" to do its work.
  This one uses the `builtin.apt` module which provides
  a wrapper around the `apt` Debian & Ubuntu package management tool.

<3> Each module then provides a bunch of parameters which control
  how it works.  Here we specify the `name` of the package we want to install ("docker")
  and tell it update its cache first, which is required on a fresh server.

Most ansible modules have pretty good documentation,
check out the `builtin.apt` one for example.
I often skip to the https://docs.ansible.com/ansible/latest/collections/ansible/builtin/apt_module.html#examples[Examples section].

[subs="specialcharacters,quotes"]

----
$ *ansible-playbook --user=elspeth -i 192.168.56.10, infra/ansible-provision.yaml -vv*
----

TODO: show ansible output.
TODO: stop using local ip 


=== SSHing Into the Server and Viewing Container Logs

Now ssh into the server, check it worked


[role="server-commands"]
[subs="specialcharacters,quotes"]
----
elspeth@server:$ *podman ps -a*
elspeth@server:$ *podman logs testcontainer*
hello, world
----

TIP: Look out for that `elspeth@server`
    in the command-line listings in this chapter.
    It indicates commands that must be run on the server,
    as opposed to commands you run on your own PC.

SSHing in to check things worked is a key server debugging skill!
It's something we want to practice on our staging server,
because ideally we'll want to avoid doing it on production machines.


=== Getting our image onto the server

Typically, you can "push" and "pull" container images
to a "container registry" -- Docker offers a public one called DockerHub,
and organisations will often run private ones,
hosted by cloud providers like AWS.

So your process of getting an image onto a server is usually
* push the image from your machine to the registry
* pull the image from the registry onto the server.
  Usually this step is implicit,
    in that you just specifying the image name
    in the format registry-url/image-name:tag,
    and then `docker run` takes care of pulling down the image for you.

But I don't want to ask you to create a DockerHub account,
or implicitly endorse any particular provider,
so we're going to "simulate" this process by doing it manually.

It turns out you can "export" a container image to an archive format,
manually copy that to the server, and then re-import it.
In ansible config, it looks like this:

[role="sourcecode"]
.infra/ansible-provision.yaml (ch11l002)
====
[source,yaml]
----
---
- hosts: all

  tasks:
    - name: Install podman
      ansible.builtin.apt:
        name: podman
        update_cache: yes
      become: true

    - name: Export container image locally
      containers.podman.podman_save:
        image: superlists
        dest: /tmp/superlists-img.oci
        format: oci-archive
        force: true
      delegate_to: 127.0.0.1

    - name: Upload image to server
      ansible.builtin.copy:
        src: /tmp/superlists-img.oci
        dest: /tmp/superlists-img.oci

    - name: Import container image on server
      containers.podman.podman_load:
        input: /tmp/superlists-img.oci

    - name: Run container
      containers.podman.podman_container:
        name: superlists
        image: superlists
        state: started
        recreate: true
----
====


Let's see if that worked!


Nope.  TODO: debug, ssh into the server, show missing env vars.


=== Using an env File to Store Our Environment Variables

We don't want to save secrets like SECRET_KEY into our ansible
config either.

* explain env files.

* explain jinja2.

[role="sourcecode"]
.infra/env.j2 (ch11l003)
====
[source,python]
----
DJANGO_DEBUG_FALSE=1
DJANGO_SECRET_KEY="{{ secret_key }}"
DJANGO_ALLOWED_HOST="{{ host }}"
----
====


[role="sourcecode"]
.infra/ansible-provision.yaml (ch11l004)
====
[source,yaml]
----


    - name: Ensure .env file exists
      ansible.builtin.template:
        src: env.j2
        dest: ~/superlists.env
      vars:
        host: "{{ inventory_hostname }}"
        secret_key: "{{ lookup('password', '/dev/null length=32 chars=ascii_letters,digits') }}"
        force: false  # do not recreate file if it already exists.

----
====

The `inventory_hostname` variable is the domain name of the server we're running against.
I'm using the `vars` section to rename it to "host", just for convenience.

* explain idempotency

* mention other secret management tools. vault??


==== More debugging

forgot ports

show ssh, curl localhosts maybe.

[role="sourcecode"]
.infra/ansible-provision.yaml (ch11l005)
====
[source,yaml]
----
    - name: Allow nonroot user to bind to port 80
      ansible.posix.sysctl:
        name: net.ipv4.ip_unprivileged_port_start
        value: 80
        reload: true
      become: true

    - name: Run container
      containers.podman.podman_container:
        name: superlists
        image: superlists
        state: started
        recreate: true
        env_file: /superlists.env
        ports: 80:8888
----
====


==== Using Systemd to Make Sure Our Container Starts on Boot

((("Systemd")))
((("Container", "automatic booting/reloading of")))
Our final step is to make sure
that the server starts up our container automatically on boot,
and reloads it automatically if it crashes.

Ansible and the podman plugins have some modules for this:


[role="sourcecode"]
.infra/ansible-provision.yaml (ch11l006)
====
[source,yaml]
----
    - name: Create container
      containers.podman.podman_container:
        name: superlists
        image: superlists
        state: stopped
        recreate: true
        env_file: ~/superlists.env
        ports: 80:8888

    - name: Generate Systemd config file
      containers.podman.podman_generate_systemd:
        name: superlists
        dest: ~/.config/systemd/user/

    - name: Container must be started and enabled on systemd
      ansible.builtin.systemd:
        name: container-superlists
        daemon_reload: true
        state: started
        enabled: true
----
====


----
vagrant@ubuntu-jammy:~$ cat ~/.config/systemd/user/container-superlists.service
# container-superlists.service
# autogenerated by Podman 3.4.4
# Wed Oct 25 10:55:38 UTC 2023

[Unit]
Description=Podman container-superlists.service
Documentation=man:podman-generate-systemd(1)
Wants=network-online.target
After=network-online.target
RequiresMountsFor=/run/user/1000/containers

[Service]
Environment=PODMAN_SYSTEMD_UNIT=%n
Restart=on-failure
TimeoutStopSec=70
ExecStart=/usr/bin/podman start superlists
ExecStop=/usr/bin/podman stop -t 10 superlists
ExecStopPost=/usr/bin/podman stop -t 10 superlists
PIDFile=/run/user/1000/containers/overlay-containers/c058e368b446388cf3b3faecdf1d8186d14d8b0a01fbf64bfca5714ae56d42fe/userdata/conmon.pid
Type=forking

[Install]
WantedBy=default.target
----

Systemd is joyously simple to configure (especially if you've ever had the
dubious pleasure of writing an `init.d` script), and is fairly
self-explanatory.


[role="small-code"]
[subs="specialcharacters,macros"]
----
$ pass:quotes[*TEST_SERVER=superlists-staging.ottg.eu python manage.py test functional_tests*]
[...]
OK
----


.More Debugging Tips and Commands
*******************************************************************************

A few more places to look and things to try, now that we've introduced
Podman and Systemd into the mix, should things not go according to plan:

- You can check the Container logs using
  `podman logs superlists`.
  ((("debugging", "Podman")))

- You can check the Systemd logs using
  `journalctl --user -u container-superlists`.
  ((("debugging", "Systemd")))


*******************************************************************************




////
old content follows


Use Vagrant to Spin Up a Local VM
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^


Running tests against the staging site gives us the ultimate confidence that
things are going to work when we go live, but we can also use a VM on our
local machine.

Download Vagrant and Virtualbox, and see if you can get Vagrant to build a
dev server on your own PC, using our Ansible playbook to deploy code to it.
Rewire the FT runner to be able to test against the local VM.

Having a Vagrant config file is particularly helpful when working
in a team--it helps new developers to spin up servers that look exactly
like yours.((("", startref="ansible29")))
////




Deploying to Live
^^^^^^^^^^^^^^^^^


So, let's try using it for our live site!

[role="small-code against-server"]
[subs=""]
----
$ <strong>fab deploy:host=elspeth@superlists.ottg.eu</strong>

Done.
Disconnecting from elspeth@superlists.ottg.eu... done.
----


'Brrp brrp brpp'. You can see the script follows a slightly different path,
doing a `git clone` to bring down a brand new repo instead of a `git pull`.
It also needs to set up a new virtualenv from scratch, including a fresh
install of pip and Django. The `collectstatic` actually creates new files this
time, and the `migrate` seems to have worked too.



Git Tag the Release
~~~~~~~~~~~~~~~~~~~


((("Git", "tagging releases")))One
final bit of admin.  In order to preserve a historical marker,
we'll use Git tags to mark the state of the codebase that reflects
what's currently live on the server:

[role="skipme"]
[subs="specialcharacters,quotes"]
----
$ *git tag LIVE*
$ *export TAG=$(date +DEPLOYED-%F/%H%M)*  # this generates a timestamp
$ *echo $TAG* # should show "DEPLOYED-" and then the timestamp
$ *git tag $TAG*
$ *git push origin LIVE $TAG* # pushes the tags up
----

Now it's easy, at any time, to check what the difference is between
our current codebase and what's live on the servers.  This will come
in useful in a few chapters, when we look at database migrations. Have
a look at the tag in the history:

[subs="specialcharacters,quotes"]
----
$ *git log --graph --oneline --decorate*
[...]
----


Anyway, you now have a live website!  Tell all your friends!  Tell your mum, if
no one else is interested!  And, in the next chapter, it's back to coding
again.((("", startref="Fstage11")))



Further Reading
~~~~~~~~~~~~~~~


((("automated deployment", "additional resources")))
There's no such thing as the One True Way in deployment,
and I'm no grizzled expert in any case.
I've tried to set you off on a reasonably sane path,
but there's plenty of things you could do differently,
and lots, lots more to learn besides.q
Here are some resources I used for inspiration:


* http://12factor.net/[The 12-factor App] by the Heroku team

* http://hynek.me/talks/python-deployments[Solid Python Deployments for Everybody] by Hynek Schlawack

* The deployment chapter of <<twoscoops,Two Scoops of Django>> by Dan
  Greenfeld and Audrey Roy




[role="pagebreak-before less_space"]
.Automated Deployments
*******************************************************************************

Idempotency::
  If your deployment script is deploying to existing servers, you need to
  design them so that they work against a fresh installation 'and' against
  a server that's already configured.
  ((("idempotency")))

Automating provisioning::
    Ultimately, _everything_ should be automated, and that includes spinning up
    brand new servers and ensuring they have all the right software installed.
    This will involve interacting with the API of your hosting provider.

Security::
  A serious discussion of server security is beyond the scope of this book,
  and I'd warn against running your own servers
  without learning a good bit more about it.
  (One reason people choose to use a PaaS to host their code
  is that it means a slightly fewer security issues to worry about.)
  If you'd like a place to start, here's as good a place as any:
  https://plusbryan.com/my-first-5-minutes-on-a-server-or-essential-security-for-linux-servers[My first 5 minutes on a server].
  I can definitely recommend the eye-opening experience of installing
  fail2ban and watching its logfiles to see just how quickly it picks up on
  random drive-by attempts to brute force your SSH login.  The internet is a
  wild place!
  ((("security issues and settings", "server security")))
  ((("Platform-As-A-Service (PaaS)")))

*******************************************************************************
