[[chapter_09_docker]]
== Deployment Part 1: Docker

[quote, 'http://bit.ly/2uhCXnH[Devops Borat]']
______________________________________________________________
Is all fun and game until you are need of put it in production.
______________________________________________________________

((("deployment", "testing using staging sites", id="DEPstage09")))
It's time to deploy the first version of our site and make it public.
They say that if you wait until you feel _ready_ to ship,
then you've waited too long.

Is our site usable? Is it better than nothing? Can we make lists on it?
Yes, yes, yes.

No, you can't log in yet.
No, you can't mark tasks as completed.
But do we really need any of that stuff?
Not really--and you can never be sure
what your users are _actually_ going to do with your site
once they get their hands on it.
We think our users want to use the site for to-do lists,
but maybe they actually want to use it
to make "top 10 best fly-fishing spots" lists,
for which you don't _need_ any kind of "mark completed" function.
We won't know until we put it out there.

Over the next couple of chapters we're going to go through
and actually deploy our site to a real, live web server.

You might be tempted to skip this bit--there's
lots of daunting stuff in it,
and maybe you think this isn't what you signed up for.
But I _strongly_ urge you to give it a go.
This is one of the sections of the book I'm most pleased with,
and it's one that people often write to me
saying they were really glad they stuck through it.

If you've never done a server deployment before,
it will demystify a whole world for you,
and there's nothing like the feeling of seeing your site live
on the actual internet.
Give it a buzzword name like "DevOps"
if that's what it takes to convince you it's worth it.

=== TDD and the Danger Areas of Deployment

Deploying a site to a live web server can be a tricky topic.
Oft-heard is the forlorn cry __"but it works on my machine!"__

((("deployment", "danger areas of")))
Some of the danger areas of deployment include:

Networking::
    Once we're off our own machine, networking issues come in:
    making sure that DNS is routing our domain
    to the correct IP address for our server,
    making sure our server is configured
    to listen to traffic coming in from the world,
    making sure it's using the right ports,
    and making sure any firewalls in the way
    are configured to let traffic through.

Dependencies::
    We need to make sure that the packages our software relies on
    (Python, Django, and so on) are installed on the server,
    and have the correct versions.

The database::
    There can be permissions and path issues,
    and we need to be careful about preserving data between deploys.

Static files (CSS, JavaScript, images, etc.)::
    Web servers usually need special configuration for serving these.
    ((("static files", "challenges of")))

Security and Configuration::
    Once we're on the public internet,
    we need to worry more about security.
    Various settings that are really useful for local development
    (like the Django debug page)
    become dangerous in production
    (because they expose our source code in tracebacks).


One way to approach the problem is to get a server,
and start manually configuring and installing everything,
hacking about until it works,
and maybe think about automating things laterfootnote:[
This was, more or less, the approach I took in earlier editions of the book.
With a fair bit of testing thrown in of course.].

But if there's one thing we've learned
in the world of agile/lean software development,
it's that taking smaller steps usually pays off.

How can we take smaller, safer steps towards a production deployment?
Can we _simulate_ the process of moving to a server,
so that we can iron out all the bugs,
before we actually take the plunge?
Can we then make small changes one at a time,
solving problems one by one,
rather than having to bite off everything in one mouthful?

Absolutely we can.  And from the title of the chapter,
I'm sure you're already guessing that Docker is going
to be part of the answer.


=== Docker, Containers and Virtualization

// TODO: experiment with moving this intro to docker
// to before the tdd danger areas bit.

// mention that ppl can skip this section if they already know obvs.

Docker is a commercial product that wraps several free
and open source technologies from the world of Linux,
sometimes referred to as "containerization".

You may have already heard of the idea of "virtualization",
which allows a single physical computer to pretend to be several machines.
Pioneered by IBM (amongst others) on mainframes in the 1960s,
it rose to mainstream adoption in the 90s,
where it was sold as a way to optimise resource usage in datacentres.
AWS, for example, was an offshoot of Amazon,
who were using virtualization already,
and realised they could sell some spare capacity on their servers
to customers outside the business.

So when you come to deploy your code to a real server in a datacentre,
it will be using virtualization.
And actually you can use virtualization on your own machine,
with software like Virtualbox or KVM.

But that can be fiddly to set up!
And nowadays, thanks to containerization, we can do better.
Because containerization is a kind of even-more-virtual virtualization.

Conceptually, "regular" virtualization works at the hardware level,
it gives you multiple virtual machines (VMs)
that pretend to be physical computers, on a single real machine.
So you can run multiple operating systems inside separate VMs
on the same physical box.

Containers work at the operating system level.
It gives you multiple virtual operating systems that
all run on a single real OS.
So you can run multiple programs inside separate virtual operating systems,
using a single real host operating system and kernel.

The upshot of this is that containers are much "cheaper".
You can start one up in milliseconds,
and you can run hundreds on the same machine.


==== Containers and your CV

That's all well and good for the _theoretical_ justification.
But let's get to the _real_ reason for using this technology,
which, as always, is:
"it's fashionable so it's going to look good on my CV."

For the purposes of this book,
that's not such a bad justification really.
Yes I think it's going to be a nice way to have a "pretend"
deployment on our own machine, before we try the real one.

But also, containers are so popular nowadays,
that it's very likely that you're going to encounter them at work
(if you haven't already).
For many working developers,
a container image is the final artifact of their work,
it's what they "deliver",
and most of the rest of the deployment process
is taken care of by someone else.


=== Docker and the danger areas of deployment

(TODO: expand this section, is just bullet points atm)

How will containerizing our software help with the danger areas?

* Containers are like a little virtual server,
  so they will force us to address many of the problems
  like dependency management and configuration.

* We can use the containers to package up as much
  of the functionality of our application as possible,
  which in turn will minimise the amount of configuration
  we need to do to our actual servers

* We can test our containers work by running our functional tests
  against them.

* Later, when we deploy our containers to a staging site,
  we can run the FTs against that too.

* If we automate container creation and deployment to staging,
  and we've tested both those things, then we will have
  minimised the risk of deployment to production.

////
old content follows. is there anything we want to rescue from here?

But there are solutions to all of these.  In order:

((("staging sites", "benefits of")))
*   Using a 'staging site', on the same infrastructure as the production site,
    can help us test out our deployments and get things right before we go to
    the "real" site.


*   We can also 'run our functional tests against the staging site'. That will
    reassure us that we have the right code and packages on the server, and
    since we now have a "smoke test" for our site layout, we'll know that the
    CSS is loaded correctly.


*   ((("virtual environment (virtualenv)", "server-based")))Just
    like on our own PC, a 'virtualenv' is useful on the server for
    managing packages and dependencies when you might be running more than one
    Python [keep-together]#application#.

*   ((("automated deployment", "benefits of")))((("automated deployment", see="also Fabric")))And
    finally, 'automation, automation, automation'.  By using an automated
    script to deploy new versions, and by using the same script to deploy to
    staging and production, we can reassure ourselves that staging is as much
    like live as possible.footnote:[What I'm calling a "staging" server, some people would
    call a "development" server, and some others would also like to distinguish
    "preproduction" servers.  Whatever we call it, the point is to have
    somewhere we can try our code out in an environment that's as similar as
    possible to the real production server.]

////

=== Our deployment procedure

Over the next few pages I'm going to go through _a_ deployment procedure.
It isn't meant to be the _perfect_ deployment procedure,
so please don't take it as being best practice,
or a recommendation--it's meant to be an illustration,
to show the kinds of issues involved in deployment,
and where testing fits in.


**This chapter: containerizing our software**

* Adapt our FTs so they can run against a container

* Build a minimal Dockerfile with everything we need to run our site,
  learn how to build and run a container on our machine,
  and run our FTs against it.

* Gradually, incrementally change the container configuration
  to make it production-ready,
  regularly runing the Fts to check we didn't break anything.

// gunicorn, DEBUG=False, secret key, etc


**Next chapter: automated deployment**

* (maybe?) ssh into the server and configure it manually first?
* figure out all the SSH permissions and DNS issues
* we'll use Ansible to build an automated script that can deploy
  our container to stagingfootnote:[
What I'm calling a "staging" server, some people would
call a "development" server, and some others would also like to distinguish
"preproduction" servers.  Whatever we call it, the point is to have
somewhere we can try our code out in an environment that's as similar as
possible to the real production server.]
* use our FTs to test staging
* then deploy to prodddd



=== As Always, Start with a Test

((("environment variables")))
((("staging sites", "adapting functional tests for", id="SSadapt09")))
Let's adapt our functional tests slightly
so that it can be run against a standalone server,
instead of the one that `LiveServerTestCase` creates for us.
We'll do it by checking for an environment variable
called `TEST_SERVER`:


[role="sourcecode"]
.functional_tests/tests.py (ch08l001)
====
[source,python]
----
import os
[...]

class NewVisitorTest(StaticLiveServerTestCase):

    def setUp(self):
        self.browser = webdriver.Firefox()
        test_server = os.environ.get('TEST_SERVER')  #<1>
        if test_server:
            self.live_server_url = 'http://' + test_server  #<2>
----
====


Do you remember I said that `LiveServerTestCase` had certain limitations?
Well, one is that it always assumes you want to use its own test server, which
it makes available at `self.live_server_url`.  I still want to be able to do
that sometimes, but I also want to be able to selectively tell it not to
bother, and to use a real server instead.

<1> The way I decided to do it is using an environment variable called
    `TEST_SERVER`.

<2> Here's the hack: we replace `self.live_server_url` with the address of
    our "real" server.

We test that said hack hasn't broken anything by running the functional
tests [keep-together]#"normally"#:

[subs="specialcharacters,macros"]
----
$ pass:quotes[*python manage.py test functional_tests*]
[...]
Ran 3 tests in 8.544s

OK
----

And now we can try them against our docker server URL,
which once we've done the right docker magic,
will be at _http://locahost:8888_

TIP: I'm deliberately choosing a different port to run Dockerised Django on (8888)
    from the default port that a local `manage.py runserver` would choose (8080),
    to avoid getting in the situation where I _think_ I (or my tests)
    are looking at Docker, when they're actually looking at a local `runserver`
    that I'd left running in some terminal somewhere.


[role="small-code"]
[subs="specialcharacters,macros"]
----
$ pass:quotes[*TEST_SERVER=localhost:8888 python manage.py test functional_tests*]

EEE
======================================================================
ERROR: test_can_start_a_list_for_one_user
(functional_tests.tests.NewVisitorTest)
 ---------------------------------------------------------------------
Traceback (most recent call last):
  File "...goat-book/functional_tests/tests.py", line 41, in
test_can_start_a_list_for_one_user
    self.browser.get(self.live_server_url)
[...]
selenium.common.exceptions.WebDriverException: Message: Reached error page: abo
ut:neterror?e=connectionFailure&u=http%3A//localhost:8888/&c=UTF-8&
f=regular&d=Firefox%20can%27t%20establish%20a%20connection%20to%20the%20server%
20at%20locahost.


======================================================================
ERROR: test_layout_and_styling (functional_tests.tests.NewVisitorTest)
 ---------------------------------------------------------------------
Traceback (most recent call last):
  File "...goat-book/functional_tests/tests.py", line 126, in
test_layout_and_styling
[...]
selenium.common.exceptions.WebDriverException: Message: Reached error page: abo
[...]


======================================================================
ERROR: test_multiple_users_can_start_lists_at_different_urls
(functional_tests.tests.NewVisitorTest)
 ---------------------------------------------------------------------
Traceback (most recent call last):
  File "...goat-book/functional_tests/tests.py", line 80, in
test_multiple_users_can_start_lists_at_different_urls
[...]
selenium.common.exceptions.WebDriverException: Message: Reached error page: abo
[...]

Ran 3 tests in 10.518s

FAILED (errors=3)
----

NOTE: If, on Windows, you see an error saying something like
    "TEST_SERVER is not recognized as a command", it's probably because
    you're not using Git-Bash.  Take another look at the
    <<pre-requisites>> section.

You can see that all the tests are failing, as expected,
since we're not running Docker yet.
Selenium reports that Firefox is seeing an error
and "cannot establish connection to the server".


((("", startref="SSadapt09")))
The FT seems to be testing the right things though, so let's commit:

[subs="specialcharacters,quotes"]
----
$ *git diff* # should show changes to functional_tests.py
$ *git commit -am "Hack FT runner to be able to test docker"*
----


TIP: Don't use `export` to set the 'TEST_SERVER' environment variable;
    otherwise, all your subsequent test runs in that terminal will be against
    staging (and that can be very confusing if you're not expecting it).
    Setting it explicitly inline each time you run the FTs is best.


==== Preparing for deployment: making a src folder


[subs="specialcharacters,quotes"]
----
$ *mkdir src*
$ *git mv * src*
$ *git commit -m "Move all our code into a src folder"*
----


=== Installing Docker


* follow instructions on web
* test with `docker run busybox` or summink.


=== A First Cut of a Dockerfile

Think of a Dockerfile as defining a brand new computer,
that we're going to use to run our django server on.
What do we need to do?  Something like this, right?

1. Install an operating system
2. Make sure it has Python on it
2. Get our source code onto it
3. Run `python manage.py runserver`


.Dockerfile
====
[source,dockerfile]
----
FROM python:slim  <1>

COPY src /src  <2>

WORKDIR /src  <3>

CMD python manage.py runserver  <4>
----
====

<1> The `FROM` line is usually the first thing in a Dockerfile,
    and it says which _base image_ we are starting from.
    Docker images are built from other Docker images!
    It's not quite turtles all the way down, but almost.
    So this is the equivalent of choosing a base operating system,
    but images can actually have lots of software preinstalled too.
    You can browse various base images on DockerHub,
    we're using one that's published by the Python Software Foundation,
    called "slim" because it's as small as possible.
    It's based on a popular version of Linux called Debian,
    and of course it comes with Python already installed on it.

<2> The `COPY` command lets you copy files
    from your own computer into the container image.
    We use it to copy all our source code from the newly-created _src_ folder,
    into a similary named folder at the root of the container image

<3> `WORKDIR` sets the current working directory for all subsequent commands.
     It's a bit like doing `cd /src`

<4> Finally the `CMD`, er, command tells docker wich, um,
    command you want it to run by default,
    when you start a container based on that image.

// deliberately wont work, django not installed


=== Building a Docker Image and Running a Docker Container

* intro to build vs run

==== Docker build

You build a container with `docker build <path-containing-dockerfile>`
and we'll use the `-t <tagname>` argument to "tag" our image
with a memorable name.

It's typical to invoke `docker build` from the folder that contains your Dockerfile,
so the last argument is usally `.`:

[subs="specialcharacters,macros"]
----
$ pass:quotes[*docker build -t superlists .*]

[+] Building 8.4s (8/8) FINISHED                            docker:default
 => [internal] load build definition from Dockerfile                  0.0s
 => => transferring dockerfile: 115B                                  0.0s
 => [internal] load .dockerignore                                     0.1s
 => => transferring context: 2B                                       0.0s
 => [internal] load metadata for docker.io/library/python:slim        0.0s
 => [internal] load build context                                     0.2s
 => => transferring context: 68.54kB                                  0.1s
 => [1/3] FROM docker.io/library/python:slim                          0.0s
 => CACHED [2/3] COPY src /src                                        0.0s
 => CACHED [3/3] WORKDIR /src                                         0.0s
 => exporting to image                                                0.0s
 => => exporting layers                                               0.0s
 => => writing image sha256:7b8e1c9fa68e7bad7994fa41e2aca852ca79f01a  0.0s
 => => naming to docker.io/library/superlists                         0.0s
----

Now we can see our image in the list of docker images on the system:

----
$ pass:quotes[*docker images*]
REPOSITORY    TAG       IMAGE ID       CREATED          SIZE
superlists    latest    7b8e1c9fa68e   13 minutes ago   155MB
----



==== Docker run


Once you've built an image,
you can run one or more containers based on that image, using `docker run`.
What happens when we run ours?
Again, we'll use the `-t` argument to find our image using its tag:


[subs="specialcharacters,macros"]
----
$ pass:quotes[*docker run superlists*]
Traceback (most recent call last):
  File "/src/manage.py", line 11, in main
    from django.core.management import execute_from_command_line
ModuleNotFoundError: No module named 'django'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/src/manage.py", line 22, in <module>
    main()
  File "/src/manage.py", line 13, in main
    raise ImportError(
ImportError: Couldn't import Django. Are you sure it's installed and available
on your PYTHONPATH environment variable? Did you forget to activate a virtual
environment?
----

TODO: note on the fact that you can't ctrl-c the process.
    explain docker -it.


Ah, we forgot that we need to install Django.


=== Virtualenv and requirements.txt

Just like on our own machine,
a virtualenv is useful in a deployed environment to make
sure we have full control over the packages installed for a particular
project.

To reproduce our local virtualenv,
rather than just manually pip installing things
one by one, and having to remember to sync things
between local dev and docker,
we can "save" the list of packages we're using
by creating a 'requirements.txt' filefootnote:[
There are many other dependency management tools these days
so requirements.txt is not the only way to do it,
although it is one of the oldest and best established.
As you continue your Python adventures
I'm sure you'll come across many others.]


[subs="specialcharacters,quotes"]
----
$ *pip freeze*
# shows all ur packages. Find django
$ *pip freeze | grep -i django== >> requirements.txt*
$ *git add requirements.txt*
$ *git commit -m "Add requirements.txt for virtualenv"*
----

You may be wondering why we didn't add our other dependency,
Selenium, to our requirements.
As always, I have to gloss over some nuance and tradeoffs,
but the short answer is that Selenium is only a dependency for the tests,
not the application code;
we're never going to run the tests directly on our production servers.

NOTE: When you have a moment,
    you might want to do some further reading
    on running your tests in Docker,
    and generating "lockfiles".

In any case, back in our Dockerfile, we can create a virtualenv
just like we did on our own machine with `python -m venv`,
and then we can use the special `-r` flag for `pip install`,
to point it at our requirements file:

.Dockerfile
====
[source,dockerfile]
----
FROM python:slim

RUN python -m venv /venv  <1>

COPY requirements.txt requirements.txt  <2>
RUN /venv/bin/pip install -r requirements.txt  <3>

COPY src /src

WORKDIR /src

CMD /venv/bin/python manage.py runserver  <4>
----
====

<1> Here's where we create our virtualenv
<2> We copy our requirements file in, just like the src folder
<3> You can't really "activate" a virtualenv inside a Dockerfile,
    so instead we provide the full path to the virtualenv version of `pip`
    when we want to do the `pip install`.
    Notice the `-r`
<4> Relatedly, we switch from using the system default Python
    to using the full path to the Python that's in our virtualenv,
    when running `manage.py`.

TIP: Forgetting the `-r` and running `pip install requirements.txt`
    is such a common error, that I recommend you do it _right now_
    and get familiar with the error message,
    because (at the time of writing), it's not that helpful.
    And it's a mistake I still make, all the time..


==== database migration

when we run it we spot a warning about migrations

[subs="specialcharacters,quotes"]
----
$ *docker run -it superlists*
You have 19 unapplied migration(s). Your project may not work properly until
you apply the migrations for app(s): auth, contenttypes, lists, sessions.
Run 'python manage.py migrate' to apply them.
----

so we add that to our dockerfile


[role="sourcecode"]
.Dockerfile
====
[source,dockerfile]
----
CMD /venv/bin/python manage.py migrate
CMD /venv/bin/python manage.py runserver
----
====

and try again

==== ports

doesnt work, show screenshot, and/or ft run.

[subs="specialcharacters,macros"]
----
$ pass:quotes[*TEST_SERVER=localhost:8888 ./manage.py test functional_tests \

selenium.common.exceptions.WebDriverException: Message: Reached error page: about:neterror[...]
----

----
CMD /venv/bin/python manage.py runserver  <4>
----



=== Using the FT to Check That Our Container Works

Let's see what our FTs think about this Docker version of our site.
I'll use the `--failfast` option to exit as soon as a single test fails:


[role="small-code"]
[subs="specialcharacters,macros"]
----
$ pass:quotes[*TEST_SERVER=localhost:8888 ./manage.py test functional_tests \
    --failfast*]
[...]
selenium.common.exceptions.WebDriverException: Message: Reached error page: [...]
----

Nope!  What's going on here?  Time for a little debugging.



=== Debugging a Container Networking Problems

First let's try and take a look ourselves, in our browser:

[[firefox-unable-to-connect-screenshot]]
.Cannot connect on that port
image::images/firefox-unable-to-connect.png["Firefox showing the 'Unable to connect' error"]


Now let's take another look at the output from our `docker run`


----
Starting development server at http://127.0.0.1:8000/
Quit the server with CONTROL-C.
----

Aha, wrong port.  Let's change that:

.Dockerfile
====
[source,dockerfile]
----
CMD /venv/bin/python manage.py runserver 8888
----

Still won't work.

////

OLD CONTENT FOLLOWS

Nope, that didn't work earlier.  Let's try an even lower-level smoke test, the
traditional Unix utility "curl" -- it's a command-line tool for making web
requests.  Try it on your own computer first:

[role='ignore-errors']
[subs="specialcharacters,quotes"]
----
$ *curl localhost:8888*
curl: (7) Failed to connect to localhost:8888: Connection
refused
----

And maybe just to be sure, we could even open up our web browser and type in
'http://localhost:8888', and confirm using a familiar tool
that things aren't working. Nope.
////


=== Running code "inside" the container with docker exec

// TODO use --name arg to docker run??

[subs="specialcharacters,quotes"]
----
$ *docker ps*  # make not of container name
$ *docker exec -it <container-name> bash*
$ *apt-get update*
Get:1 http://deb.debian.org/debian bookworm InRelease [151 kB]
Get:2 http://deb.debian.org/debian bookworm-updates InRelease [52.1 kB]
Get:3 http://deb.debian.org/debian-security bookworm-security InRelease [48.0 kB]
[...]
$ *apt-get install curl*
Reading package lists... Done
Building dependency tree... Done
Reading state information... Done
The following additional packages will be installed:
  libbrotli1 libcurl4 libldap-2.5-0 libldap-common libnghttp2-14 libpsl5
librtmp1 libsasl2-2 libsasl2-modules
  libsasl2-modules-db libssh2-1 publicsuffix
[...]
$ *curl -iv http://localhost:8000*
*   Trying 127.0.0.1:8000...
* Connected to localhost (127.0.0.1) port 8000 (#0)
> GET / HTTP/1.1
> Host: localhost:8000
> User-Agent: curl/7.88.1
> Accept: */*
>
< HTTP/1.1 200 OK
HTTP/1.1 200 OK
[...]
<!doctype html>
<html lang="en">

  <head>
    <title>To-Do lists</title>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link href="/static/bootstrap/css/bootstrap.min.css" rel="stylesheet">
  </head>

  <body>
[...]
----

OK so we can see django is serving our site _inside_ the container,
why can't we see it _outside_??

Basically we need to do two things:

==== exposing docker ports

docker runs in its own little world,
need to expose ports.  explain A:B syntax.

[subs="specialcharacters,quotes"]
----
$ *podman run -p 8888:8888 -it superlists*
----

==== Essential Googling the Error Message

the `-p` argument to `docker run` is something you just learn,
fairly on in learning docker.
But the next debugging step is a fair bit more obscure.
We'll have to resort to the tried and tested
"Googling the error message" technique instead
(<<googling-the-error>>).


[[googling-the-error]]
.An indispensable publication (source: https://news.ycombinator.com/item?id=11459601[])
image::images/orly-essential-googling-the-error-message.png["Cover of a fake O'Reilly book called Googling the Error Message",400]


Everyone's search results are a little different,
and mine are perhaps shaped by years of working with docker and django,
but I found the answer in my very first result
(see <<google-results-screenshot>>),
which was a https://stackoverflow.com/questions/49476217/docker-cant-access-django-server[stackoverflow post].


[[google-results-screenshot]]
.Google can still deliver results
image::images/google-results-with-stackoverflow.png["Google results with a useful stackoverflow post in first position",400]


So we need to tell django to bind to any IP address,
because container networking doesn't always have 127.0.0.1
as the address of _localhost_:

.Dockerfile
====
[source,dockerfile]
----
CMD /venv/bin/python manage.py runserver 0.0.0.0:8888
----

.On Debugging
*******************************************************************************
Let me let you in on a little secret.  I'm actually bad at debugging.
We all have our psychological strengths and weakness,
and one of my weaknesses is
that when I run into a problem I can't see an obvious solution to,
I want to throw up my hands way too soon
and say "well, this is hopeless, it can't be fixed",
and give up.

Thankfully I have had some good role models over the years
who are much better at it than me (hi Glenn!).
Debugging needs the patience and tenacity of a bloodhound.
If at first you don't succeed,
you need to systematically rule out options,
check your assumptions,
eliminate various aspects of the problem and simplify things down,
find the parts that do and don't work,
until you eventually find the cause.

It always seems hopeless at first!  But eventually you get there.

*******************************************************************************



Let's try our FTs again:


[role="small-code"]
[subs="specialcharacters,macros"]
----
$ pass:quotes[*TEST_SERVER=superlists-staging.ottg.eu:8000 ./manage.py test functional_tests \
    --failfast*]
Found 3 test(s).
Creating test database for alias 'default'...
System check identified no issues (0 silenced).
...
 ---------------------------------------------------------------------
Ran 3 tests in 26.965s

OK
----



AMAZING IT ACTUALLY WORKS

commit commit commit.


TODO: maybe move all this lot to a "making production-ready" chapter after all??


[role="small-code"]
[subs="specialcharacters,macros"]
----
======================================================================
FAIL: test_can_start_a_list_for_one_user
(functional_tests.tests.NewVisitorTest)
 ---------------------------------------------------------------------
Traceback (most recent call last):
  File "...goat-book/functional_tests/tests.py", line 44, in
test_can_start_a_list_for_one_user
    self.assertIn('To-Do', self.browser.title)
AssertionError: 'To-Do' not found in 'DisallowedHost at /'
 ---------------------------------------------------------------------
Ran 1 test in 4.010s

FAILED (failures=1)
[...]
----

NOTE: At this point, if your FTs still can't talk to the server,
    something else must be in the way.  Check your provider's firewall
    settings, and make sure ports 80 and 8000 are open to the world. On AWS,
    for example, you may need to configure the "security group" for your
    server.

Oops, spoke too soon!  Another error.  We didn't look closely enough at
that `curl` pass:[<span class="keep-together">output</span>]...


* TODO: this next bit may not happen yet

Hacking ALLOWED_HOSTS in settings.py
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Don't be disheartened!  We may have just fixed one problem only to run straight
into another, but this problem is definitely a much easier one.  At least we
can talk to the server!  And it's giving us a helpful pointer.  Try opening the
site manually (<<django-disallowedhosts-error>>):

[[django-disallowedhosts-error]]
.Another hitch along the way
image::images/twp2_0902a.png["the Django debug page explaining the DisallowedHost error"]


`ALLOWED_HOSTS` is a security setting designed to reject requests that are
likely to be forged, broken or malicious because they don't appear to be
asking for your site (HTTP request contain the address they were intended for
in a header called "Host").

By default, when DEBUG=True, `ALLOWED_HOSTS` effectively allows _localhost_,
our own machine, so that's why it was working OK in dev, and from the server
itself (where we ask for 'localhost'), but not from our own machine (where we
ask for 'superlists-staging.ottg.eu')

There's more information in the http://bit.ly/2u0R2d6[Django docs].

The upshot is that we need to adjust `ALLOWED_HOSTS` in 'settings.py'. Since
we're just hacking for now, let's set it to the totally insecure allow-everyone
"*" setting:

[role="sourcecode"]
.superlists/settings.py
====
[source,python]
----
# SECURITY WARNING: don't run with debug turned on in production!
DEBUG = True

ALLOWED_HOSTS = ['*']
[...]
----
====


And pull it down on the server, and restart our `runserver` process:

[role="server-commands"]
[subs="specialcharacters,quotes"]
----
elspeth@server:$ *git pull*
elspeth@server:$ *./.venv/bin/python manage.py runserver 0.0.0.0:8000*
----

A quick visual inspection confirms--the site is up (<<staging-is-up>>)!

[[staging-is-up]]
.The staging site is up!
image::images/twp2_0903.png["The front page of the site, at least, is up"]


Let's see what our functional tests say:

[role="small-code"]
[subs="specialcharacters,macros"]
----
$ pass:quotes[*TEST_SERVER=superlists-staging.ottg.eu:8000 ./manage.py test functional_tests \
    --failfast*]
[...]
selenium.common.exceptions.NoSuchElementException: Message: Unable to locate
element: [id="id_list_table"]
----


The tests are failing as soon as they try to submit a new item, because we
haven't set up the database. You'll probably have spotted the yellow Django
debug page (<<django-debug-screen>>) telling us as much as the tests went
through, or if you tried it manually.


NOTE: The tests saved us from potential embarrassment there.  The site 'looked'
    fine when we loaded its front page.  If we'd been a little hasty and only
    testing manually, we might have thought we were done, and it would have
    been the first users that discovered that nasty Django DEBUG page.  Okay,
    slight exaggeration for effect, maybe we 'would' have checked, but what
    happens as the site gets bigger and more complex? You can't check
    everything. The tests can.

[[django-debug-screen]]
.But the database isn't
image::images/twp2_0904.png["Django DEBUG page showing database error"]




Creating the Database with migrate
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

((("database migrations")))We
run `migrate` using the `--noinput` argument to suppress the two little "are
you sure" prompts:

[role="server-commands"]
[subs="specialcharacters,quotes"]
----
elspeth@server:$ *./.venv/bin/python manage.py migrate --noinput*
Operations to perform:
  Apply all migrations: auth, contenttypes, lists, sessions
Running migrations:
  Applying contenttypes.0001_initial... OK
  [...]
  Applying lists.0004_item_list... OK
  Applying sessions.0001_initial... OK
----

That looks good.  We restart the server:


[role="server-commands"]
[subs="specialcharacters,quotes"]
----
elspeth@server:$ *./.venv/bin/python manage.py runserver 0.0.0.0:8000*
----

And try the FTs again:

[role="small-code"]
[subs="specialcharacters,macros"]
----
$ pass:quotes[*TEST_SERVER=superlists-staging.ottg.eu:8000 ./manage.py test functional_tests*]
[...]

...
 ---------------------------------------------------------------------
Ran 3 tests in 10.718s

OK
----

Hooray, that's a working deploy!

Time for a well-earned tea break I think, and perhaps a
https://en.wikipedia.org/wiki/Digestive_biscuit[chocolate biscuit].


Success!  Our Hack Deployment Works
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Phew.  Well, it took a bit of hacking about, but now we can be reassured that
the basic piping works. Notice that the FT was able to guide us incrementally
towards a working site.

But we really can't be using the Django dev server in production, or running on
port 8000 forever. In the next chapter, we'll make our hacky deployment more
production-ready.((("", startref="DEPstage09")))


((("deployment", "getting to production-ready", id="DPprodready10")))Our
deployment is working fine but it's not production-ready.  Let's try
to get it there, using the tests to guide us.

In a way we're applying the Red-Green-Refactor cycle to our server deployment.
Our hacky deployment got us to Green, and now we're going to Refactor, working
incrementally (just as we would while coding), trying to move from working
state to working state, and using the FTs to detect any regressions.

////
I'm planning to host my staging server at 'superlists-staging.ottg.eu':


NOTE: A clarification: in this chapter, we run tests 'against' our staging
    server, not 'on' our staging server.  So we still run the tests from our
    own laptop, but they target the site that's running on the server.
////





What We Need to Do
~~~~~~~~~~~~~~~~~~

What's wrong with our hacky deployment?  A few things: first, we need to host
our app on the "normal" port 80 so that people can access it using a regular
URL.

Perhaps more importantly, we shouldn't use the Django dev server for
production; it's not designed for real-life workloads.  Instead, we'll use the
popular combination of the Nginx web server and the Gunicorn Python/WSGI
server.

((("DEBUG settings")))Several settings in 'settings.py' are currently
unacceptable too. `DEBUG=True`, is strongly recommended against for production,
and we'll want to fix `ALLOWED_HOSTS`, and set a unique `SECRET_KEY` too.

Finally, we don't want to have to SSH in to our server to actually start the site.
Instead,  we'll write a Systemd config file so that it starts up automatically
whenever the server (re)boots.

Let's go through and see if we can fix each of these things one by one.



Switching to Gunicorn
~~~~~~~~~~~~~~~~~~~~~

((("production-ready deployment", "using Gunicorn", secondary-sortas="Gunicorn")))
((("Gunicorn", "switching to")))
Do you know why the Django mascot is a pony?  The story is that Django
comes with so many things you want: an ORM, all sorts of middleware,
the admin site... "What else do you want, a pony?" Well, Gunicorn stands
for "Green Unicorn", which I guess is what you'd want next if you already
had a pony...

----
pip install gunicorn
pip freeze | grep -i gunicorn >> requirements.txt
----

Gunicorn will need to know a path to a WSGI server, which is usually
a function called `application`.  Django provides one in 'superlists/wsgi.py':

----
CMD ["/venv/bin/gunicorn", "--bind", ":80", "superlists.wsgi:application"]
----


But if we run the functional tests, once again you'll see that they are
warning us of a problem. The test for adding list items passes happily, but the
test for layout + styling fails.  Good job, tests!

[role="small-code"]
[subs="specialcharacters,macros"]
----
$ pass:quotes[*STAGING_SERVER=superlists-staging.ottg.eu python manage.py test functional_tests*]
[...]
AssertionError: 117.0 != 512 within 10 delta
FAILED (failures=1)
----

And indeed, if you take a look at the site, you'll find the CSS is all broken,
as in <<site-with-broken-css>>.

The reason that the CSS is broken is that although the Django dev server will
serve static files magically for you, Gunicorn doesn't.



[[site-with-broken-css]]
.Broken CSS
image::images/twp2_1001.png["The site is up, but CSS is broken"]


One step forward, one step backward, but once again we've identified the
problem nice and early. Moving on!

////
TIP: At this point if you see a "502 - Bad Gateway", it's probably because you
    forgot to restart Gunicorn.
////


=== Static Files with Whitenoise


And if you take another manual look at your site, things should look much
healthier. Let's rerun our FTs:

[role="small-code"]
[subs="specialcharacters,macros"]
----
$ pass:quotes[*STAGING_SERVER=superlists-staging.ottg.eu python manage.py test functional_tests*]
[...]

...
 ---------------------------------------------------------------------
Ran 3 tests in 10.718s

OK
----

Phew.



=== Using Environment Variables to Adjust Settings for Production

((("DEBUG settings")))
((("production-ready deployment", "DEBUG=false and ALLOWED_HOSTS")))
((("ALLOWED_HOSTS")))
((("security issues and settings", "ALLOWED_HOSTS")))
((("tracebacks")))
We know there are several things in
_settings.py_ that we want to change for production:


* +ALLOWED_HOSTS+ is currently set to "*" which isn't secure.  We want it
  to be set to only match the site we're supposed to be serving
  (_localhost_ for now, but someday soon, a real domain).

* +DEBUG+ mode is all very well for hacking about on your own server, but
  leaving those pages full of tracebacks available to the world
  https://docs.djangoproject.com/en/1.11/ref/settings/#debug[isn't secure].

* `SECRET_KEY` is used by Django for some of its crypto--things like cookies
  and CSRF protection. It's good practice to make sure the secret key on the
  server is different from the one in your source code repo, because that code
  might be visible to strangers.  We'll want to generate a new, random one but
  then keep it the same for the foreseeable future (find out more in the
  https://docs.djangoproject.com/en/1.11/topics/signing/[Django docs]).

Development, staging and live sites always have some differences
in their configuration. Environment variables are a good place to
store those different settings.  See
http://www.clearlytech.com/2014/01/04/12-factor-apps-plain-english/["the
12-factor app"].footnote:[
Another common way of handling this is to have different versions of
_settings.py_ for dev and prod.  That can work fine too, but it can
get confusing to manage.  Environment variables also have the advantage
of working for non-Django stuff too...]


Here's one way to make it work:


[role="sourcecode"]
.superlists/settings.py (ch08l004)
====
[source,python]
----
if 'DJANGO_DEBUG_FALSE' in os.environ:  #<1>
    DEBUG = False
    SECRET_KEY = os.environ['DJANGO_SECRET_KEY']  #<2>
    ALLOWED_HOSTS = [os.environ['SITENAME']]  #<2>
else:
    DEBUG = True  #<3>
    SECRET_KEY = 'insecure-key-for-dev'
    ALLOWED_HOSTS = []
----
====

<1> We say we'll use an environment variable called `DJANGO_DEBUG_FALSE`
    to switch debug mode off, and in effect require production settings
    (it doesn't matter what we set it to, just that it's there).

<2> And now we say that, if debug mode is off, we _require_ the
    `SECRET_KEY` and `ALLOWED_HOSTS` to be set by two more environment
    variables (one of which can be the `$SITENAME` variable we've been
    using at the command-line so far).

<3> Otherwise we fall-back to the insecure, debug mode settings that
    are useful for Dev.

There are other ways you might set up the logic, making various variables
optional, but I think this gives us a little bit of protection against
accidentally forgetting to set one.  The end result is that you don't
need to set any of them for dev, but production needs all three, and it
will error if any are missing.

TIP: Better to fail hard than allow a typo in an environment variable name to
    leave you running with insecure settings.

Let's do our usual dance of committing locally, and pushing to GitHub:

[role="small-code"]
[subs="specialcharacters,quotes"]
----
$ *git commit -am "use env vars for prod settings DEBUG, ALLOWED_HOSTS, SECRET_KEY"*
$ *git push*
----

Then pull it down on the server, export a couple of environment variables,
and restart Gunicorn:

[role="server-commands"]
[subs="specialcharacters,quotes"]
----
ENV DJANGO_DEBUG_OFF=1
----


And use a test run to reassure ourselves that things still work...

[role="small-code"]
[subs="specialcharacters,macros"]
----
$ pass:quotes[*STAGING_SERVER=superlists-staging.ottg.eu ./manage.py test functional_tests --failfast*]
[...]
AssertionError: 'To-Do' not found in ''
----

Oops.  Let's take a look manually: <<django-400-error>>.

[[django-400-error]]
.An ugly 400 error
image::images/twp2_1002.png["An unfriendly page showing 400 Bad Request"]


=== allowed hosts

Something's gone wrong.  But once again, by running our FTs frequently,
we're able to identify the problem early, before we've changed too many things.
In this case the only thing we've changed is _settings.py_. We've changed three
settings—which one might be at fault?

Let's use the "Googling the error message" technique again.

The very first link in my search results for
https://www.google.co.uk/?q=django+400+bad+request[Django 400 Bad Request] suggests that a 400 error is usually to do with `ALLOWED_HOSTS`.  In the
last chapter we had a nice Django Debug page saying "DisallowedHost error"
(<<django-disallowedhosts-error>>), but now because we have `DEBUG=False`, we
just get the minimal, unfriendly 400 page.

But what's wrong with `ALLOWED_HOSTS`? After double-checking it for typos, we
might do a little more Googling with some relevant keywords:
https://www.google.co.uk/search?q=django+allowed+hosts+nginx[Django
ALLOWED_HOSTS Nginx]. Once again, the
https://www.digitalocean.com/community/questions/bad-request-400-django-nginx-gunicorn-on-debian-7[first result]
gives us the clue we need.


Fixing ALLOWED_HOSTS with Nginx: passing on the Host header
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The problem turns out to be that, by default, Nginx strips out the Host
headers from requests it forwards, and it makes it "look like" they came
from _localhost_ after all.  We can tell it to forward on the original host
header by adding the `proxy_set_header` directive:


[role="sourcecode"]
.server: /etc/nginx/sites-available/superlists-staging.ottg.eu
====
[source,nginx]
----
server {
    listen 80;
    server_name superlists-staging.ottg.eu;

    location /static {
        alias /home/elspeth/sites/superlists-staging.ottg.eu/static;
    }

    location / {
        proxy_pass http://unix:/tmp/superlists-staging.ottg.eu.socket;
        proxy_set_header Host $host;
    }
}
----
====

Reload Nginx once more:

[role="server-commands"]
[subs="specialcharacters,quotes"]
----
elspeth@server:$ *sudo systemctl reload nginx*
----

And then we try our FTs again:


[role="small-code"]
[subs="specialcharacters,macros"]
----
$ pass:quotes[*STAGING_SERVER=superlists-staging.ottg.eu python manage.py test functional_tests*]
[...]
OK
----


Phew.  Back to working again.


Using a .env File to Store Our Environment Variables
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Another little refactor.  Setting environment variables manually in various
shells is a pain, and it'd be nice to have them all available in a single
place.  The Python world (and other people out there too) seems to be
standardising around using the convention of a file called _.env_ in the
project root.

First we add _.env_ to our __.gitignore__—this file is going to be used
for secrets, and we don't ever want them ending up on GitHub:


[subs="specialcharacters,quotes"]
----
$ *echo .env >> .gitignore*
$ *git commit -am"gitignore .env file"*
$ *git push*
----


Next let's save our environment on the server:

[role="server-commands"]
[subs="specialcharacters,quotes"]
----
elspeth@server:$ *pwd*
/home/elspeth/sites/superlists-staging.ottg.eu
elspeth@server:$ *echo DJANGO_DEBUG_FALSE=y >> .env*
elspeth@server:$ *echo SITENAME=$SITENAME >>.env*
----


NOTE: The way I've used the environment files in _settings.py_ means
    that the _.env_ file is not required on your own machine, only
    in staging/production.


Generating a secure SECRET_KEY
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

While we're at it we'll also generate a more secure secret key using a little
Python one-liner.


[role="server-commands"]
[subs=""]
----
elspeth@server:$ <strong>echo DJANGO_SECRET_KEY=$(
python3.7 -c"import random; print(''.join(random.SystemRandom().
choices('abcdefghijklmnopqrstuvwxyz0123456789', k=50)))"
) &gt;&gt; .env</strong>
elspeth@server:$ <strong>cat .env</strong>
DJANGO_DEBUG_FALSE=y
SITENAME=superlists-staging.ottg.eu
DJANGO_SECRET_KEY=[...]
----

Now let's check our env file works, and restart gunicorn:


[role="server-commands"]
[subs="specialcharacters,quotes"]
----
elspeth@server:$ *unset DJANGO_SECRET_KEY DJANGO_DEBUG_FALSE SITENAME*
elspeth@server:$ *echo $DJANGO_DEBUG_FALSE-none*
-none
elspeth@server:$ *set -a; source .env; set +a*
elspeth@server:$ *echo $DJANGO_DEBUG_FALSE-none*
y-none
elspeth@server:$ *./.venv/bin/gunicorn --bind \
    unix:/tmp/$SITENAME.socket superlists.wsgi:application*
----


And we rerun our FTs to check that they agree, everything still works:

[role="small-code"]
[subs="specialcharacters,macros"]
----
$ pass:quotes[*STAGING_SERVER=superlists-staging.ottg.eu python manage.py test functional_tests*]
[...]
OK
----


Excellent!  That went without a hitch :)


TIP: I've shown the use of a _.env_ file and manually extracting environment
    variables in _settings.py_, but there are some plugins that do this stuff
    for you that are definitely worth investigating.  Look into
    https://django-environ.readthedocs.io/en/latest/[django-environ],
    https://github.com/jpadilla/django-dotenv[django-dotenv], and
    https://docs.pipenv.org/[Pipenv].



Using Systemd to Make Sure Gunicorn Starts on Boot
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


((("production-ready deployment", "using Systemd for automatic booting/reloading")))((("Systemd")))((("Gunicorn", "automatic booting/reloading of")))Our
final step is to make sure that the server starts up Gunicorn automatically
on boot, and reloads it automatically if it crashes. On Ubuntu, the way to do
this is using Systemd.

Here's what a Systemd config file looks like

[role="sourcecode small-code"]
.server: /etc/systemd/system/gunicorn-superlists-staging.ottg.eu.service
====
[source,bash]
----
[Unit]
Description=Gunicorn server for superlists-staging.ottg.eu

[Service]
Restart=on-failure  <1>
User=elspeth  <2>
WorkingDirectory=/home/elspeth/sites/superlists-staging.ottg.eu  <3>
EnvironmentFile=/home/elspeth/sites/superlists-staging.ottg.eu/.env  <4>

ExecStart=/home/elspeth/sites/superlists-staging.ottg.eu/.venv/bin/gunicorn \
    --bind unix:/tmp/superlists-staging.ottg.eu.socket \
    superlists.wsgi:application  <5>

[Install]
WantedBy=multi-user.target <6>
----
====

Systemd is joyously simple to configure (especially if you've ever had the
dubious pleasure of writing an `init.d` script), and is fairly
self-explanatory.

<1> `Restart=on-failure` will restart the process automatically if it crashes.

<2> `User=elspeth` makes the process run as the "elspeth" user.

<3> `WorkingDirectory` sets the current working directory.

<4> `EnvironmentFile` points Systemd towards our _.env_ file and tells it
    to load environment variables from there.

<5> `ExecStart` is the actual process to execute.  I'm using the ++\++ line
    continuation characters to split the full command over multiple lines,
    for readability, but it could all go on one line.

<6> `WantedBy` in the `[Install]` section is what tells Systemd we want this
    service to start on boot.

Systemd scripts live in '/etc/systemd/system', and their names must end in
'.service'.

Now we tell Systemd to start Gunicorn with the `systemctl` command:

[role="server-commands"]
[subs="specialcharacters,quotes"]
----
# this command is necessary to tell Systemd to load our new config file
elspeth@server:$ *sudo systemctl daemon-reload*
# this command tells Systemd to always load our service on boot
elspeth@server:$ *sudo systemctl enable gunicorn-superlists-staging.ottg.eu*
# this command actually starts our service
elspeth@server:$ *sudo systemctl start gunicorn-superlists-staging.ottg.eu*
----

(You should find the `systemctl` command responds to tab completion, including
of the service name, by the way.)

Now we can rerun the FTs to see that everything still works. You can even test
that the site comes back up if you reboot the server!

[role="small-code"]
[subs="specialcharacters,macros"]
----
$ pass:quotes[*STAGING_SERVER=superlists-staging.ottg.eu python manage.py test functional_tests*]
[...]
OK
----


.More Debugging Tips and Commands
*******************************************************************************

A few more places to look and things to try, now that we've introduced
Gunicorn and Systemd into the mix, should things not go according to plan:

- ((("debugging", "Systemd")))You can check the Systemd logs using
  `sudo journalctl -u gunicorn-superlists-staging.ottg.eu`.

- You can ask Systemd to check the validity of your service configuration:
  `systemd-analyze verify /path/to/my.service`.

- Remember to restart both services whenever you make changes.

- If you make changes to the Systemd config file, you need to
  run `daemon-reload` before `systemctl restart` to see the effect
  of your changes.

*******************************************************************************



Saving Our Changes: Adding Gunicorn to Our requirements.txt
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

((("requirements.txt")))((("Gunicorn", "adding to requirements.txt")))Back
in the 'local' copy of your repo, we should add Gunicorn to the list
of packages we need in our virtualenvs:

[subs="specialcharacters,quotes"]
----
$ *pip install gunicorn*
$ *pip freeze | grep -i gunicorn >> requirements.txt*
$ *git commit -am "Add gunicorn to virtualenv requirements"*
$ *git push*
----


NOTE: ((("Windows", "Gunicorn support")))On
    Windows, at the time of writing, Gunicorn would `pip install` quite
    happily, but it wouldn't actually work if you tried to use it.  Thankfully
    we only ever run it on the server, so that's not a problem. And, Windows
    support is
    http://stackoverflow.com/questions/11087682/does-gunicorn-run-on-windows[being discussed]...





Thinking About Automating
~~~~~~~~~~~~~~~~~~~~~~~~~

((("production-ready deployment", "preparing for automation", id="PRDauto10")))((("automated deployment", "preparing for")))Let's
recap our provisioning and deployment procedures:

Provisioning::
1. Assume we have a user account and home folder
2. `add-apt-repository ppa:deadsnakes/ppa && apt update`
3. `apt install nginx git python3.7 python3.7-venv`
4. Add Nginx config for virtual host
5. Add Systemd job for Gunicorn (including unique +SECRET_KEY+)

Deployment::
1. Create directory in '~/sites'
2. Pull down source code
3. Start virtualenv in 'virtualenv'
4. `pip install -r requirements.txt`
5. `manage.py migrate` for database
6. `collectstatic` for static files
7. Restart Gunicorn job
8. Run FTs to check everything works


Assuming we're not ready to entirely automate our provisioning process, how
should we save the results of our investigation so far?  I would say that
the Nginx and Systemd config files should probably be saved somewhere, in
a way that makes it easy to reuse them later.  Let's save them in a new
subfolder in our repo.


Saving Templates for Our Provisioning Config Files
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

((("templates", "saving for provisioning config files", id="TMPprovision10")))First,
 we create the subfolder:

[subs="specialcharacters,quotes"]
----
$ *mkdir deploy_tools*
----

[role="pagebreak-before"]
Here's a generic template for our Nginx config:

[role="sourcecode"]
.deploy_tools/nginx.template.conf
====
[source,nginx]
----
server {
    listen 80;
    server_name DOMAIN;

    location /static {
        alias /home/elspeth/sites/DOMAIN/static;
    }

    location / {
        proxy_pass http://unix:/tmp/DOMAIN.socket;
        proxy_set_header Host $host;
    }
}
----
====

And here's one for the Gunicorn Sytemd service:

[role="sourcecode"]
.deploy_tools/gunicorn-systemd.template.service
====
[source,bash]
----
[Unit]
Description=Gunicorn server for DOMAIN

[Service]
Restart=on-failure
User=elspeth
WorkingDirectory=/home/elspeth/sites/DOMAIN
EnvironmentFile=/home/elspeth/sites/DOMAIN/.env

ExecStart=/home/elspeth/sites/DOMAIN/.venv/bin/gunicorn \
    --bind unix:/tmp/DOMAIN.socket \
    superlists.wsgi:application

[Install]
WantedBy=multi-user.target
----
====

Now it's easy for us to use those two files to generate
a new site, by doing a find and replace on `DOMAIN`.

For the rest, just keeping a few notes is OK. Why not keep
them in a file in the repo too?

[role="sourcecode"]
.deploy_tools/provisioning_notes.md
====
[source,rst]
----
Provisioning a new site
=======================

## Required packages:

* nginx
* Python 3.6
* virtualenv + pip
* Git

eg, on Ubuntu:

    sudo add-apt-repository ppa:deadsnakes/ppa
    sudo apt update
    sudo apt install nginx git python3.7 python3.7-venv

## Nginx Virtual Host config

* see nginx.template.conf
* replace DOMAIN with, e.g., staging.my-domain.com

## Systemd service

* see gunicorn-systemd.template.service
* replace DOMAIN with, e.g., staging.my-domain.com

## Folder structure:

Assume we have a user account at /home/username

/home/username
└── sites
    ├── DOMAIN1
    │    ├── .env
    │    ├── db.sqlite3
    │    ├── manage.py etc
    │    ├── static
    │    └── virtualenv
    └── DOMAIN2
         ├── .env
         ├── db.sqlite3
         ├── etc
----
====

We can do a commit for those:

[subs="specialcharacters,quotes"]
----
$ *git add deploy_tools*
$ *git status* # see three new files
$ *git commit -m "Notes and template config files for provisioning"*
----

[role="pagebreak-before"]
((("", startref="PRDauto10")))((("", startref="TMPprovision10")))Our
source tree will now look something like this:

[subs=""]
----
.
├── deploy_tools
│   ├── gunicorn-systemd.template.service
│   ├── nginx.template.conf
│   └── provisioning_notes.md
├── functional_tests
│   ├── [...]
├── lists
│   ├── __init__.py
│   ├── models.py
│   ├── [...]
│   ├── static
│   │   ├── base.css
│   │   └── bootstrap
│   │       ├── [...]
│   ├── templates
│   │   ├── base.html
│   │   ├── [...]
│   ├── tests.py
│   ├── urls.py
│   └── views.py
├── manage.py
├── requirements.txt
├── static
│   ├── [...]
├── superlists
│   ├── [...]
└── virtualenv
    ├── [...]

----



Saving Our Progress
~~~~~~~~~~~~~~~~~~~


Being able to run our FTs against a staging server can be very reassuring.
But, in most cases, you don't want to run your FTs against your "real" server.
In order to "save our work", and reassure ourselves that the production server
will work just as well as the real server, we need to make our deployment
process repeatable.((("", startref="DPprodready10")))

Automation is the answer, and it's the topic of the next chapter.

[role="pagebreak-before less_space"]
.Production-Readiness for Server Deployments
*******************************************************************************

((("production-ready deployment", "best practices for")))A
few things to think about when trying to build a production-ready server
[keep-together]#environment#:

Don't use the Django dev server in production::
    ((("Gunicorn", "benefits of")))Something
    like Gunicorn or uWSGI is a better tool for running Django; it
    will let you run multiple workers, for example.

Don't use Django to serve your static files::
    ((("static files", "serving with Nginx")))There's
    no point in using a Python process to do the simple job of serving
    static files. Nginx can do it, but so can other web servers like Apache or
    uWSGI.

Check your settings.py for dev-only settings::
    `DEBUG=True`, `ALLOWED_HOSTS` and `SECRET_KEY` are the ones we came across,
    but you will probably have others (we'll see more when we start to send
    emails from the server).

Security::
    ((("security issues and settings", "server security")))((("Platform-As-A-Service (PaaS)")))A
    serious discussion of server security is beyond the scope of this book,
    and I'd warn against running your own servers without learning a good bit
    more about it. (One reason people choose to use a PaaS to host their
    code is that it means a slightly fewer security issues to worry about.)
    If you'd like a place to start, here's as good a place as any:
    https://plusbryan.com/my-first-5-minutes-on-a-server-or-essential-security-for-linux-servers[My first 5 minutes on a server].
    I can definitely recommend the eye-opening experience of installing
    fail2ban and watching its logfiles to see just how quickly it picks up on
    random drive-by attempts to brute force your SSH login.  The internet is a
    wild place!

*******************************************************************************

.Test-Driving Server Configuration and Deployment
*******************************************************************************

Tests take some of the uncertainty out of deployment::
    ((("staging sites", "benefits of")))For
    developers, server administration is always "fun", by which I mean, a
    process full of uncertainty and surprises. My aim during this chapter was
    to show that a functional test suite can take some of the uncertainty out
    of the process.

Some typical pain points--networking, ports, static files, and the database::
    The things that you need to keep an eye out for on any deployment include
    making sure your database configuration, static files, software
    dependencies, and custom settings that differ between development and
    production.  You'll need to think through each of these for your own
    deployments.

Tests allow us to experiment and work incrementally::
    Whenever we make a change to our server configuration, we can rerun the
    test suite, and be confident that everything works as well as it did
    before.  It allows us to experiment with our setup with less fear (as
    we'll see in the next chapter).

*******************************************************************************
